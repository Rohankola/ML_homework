{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13pL--6rycN3"
      },
      "source": [
        "## Homework02: Three headed network in PyTorch\n",
        "\n",
        "This notebook accompanies the [week02](https://github.com/girafe-ai/natural-language-processing/tree/master/week02_cnn_for_texts) practice session. Refer to that notebook for more comments.\n",
        "\n",
        "All the preprocessing is the same as in the classwork. *Including the data leakage in the train test split (it's still for bonus points).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P8zS7m-gycN5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import nltk\n",
        "import tqdm\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_ZGoHRR1nKT"
      },
      "source": [
        "If you have already downloaded the data on the Seminar, simply run through the next cells. Otherwise uncomment the next cell (and comment the another one ;)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "uG0TZatd1nKU"
      },
      "outputs": [],
      "source": [
        "# uncomment and run this cell, if you don't have data locally yet.\n",
        "\n",
        "#!curl -L \"https://www.dropbox.com/s/5msc5ix7ndyba10/Train_rev1.csv.tar.gz?dl=1\" -o Train_rev1.csv.tar.gz\n",
        "#!tar -xvzf ./Train_rev1.csv.tar.gz\n",
        "\n",
        "data = pd.read_csv(\"./Train_rev1.csv\", index_col=None)\n",
        "\n",
        "#!wget https://raw.githubusercontent.com/girafe-ai/natural-language-processing/22f_msai/homeworks/assignment02_three_headed_network/network.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "UuuKIKfrycOH"
      },
      "outputs": [],
      "source": [
        "data['Log1pSalary'] = np.log1p(data['SalaryNormalized']).astype('float32')\n",
        "text_columns = [\"Title\", \"FullDescription\"]\n",
        "categorical_columns = [\"Category\", \"Company\", \"LocationNormalized\", \"ContractType\", \"ContractTime\"]\n",
        "target_column = \"Log1pSalary\"\n",
        "\n",
        "data[categorical_columns] = data[categorical_columns].fillna('NaN') # cast missing values to string \"NaN\"\n",
        "\n",
        "data.sample(3)\n",
        "\n",
        "\n",
        "data_for_autotest = data[-5000:]\n",
        "data = data[:-5000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUWkpd7PycOQ",
        "outputId": "fca4dc6a-320c-43b0-9815-e61fce8996bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized:\n",
            "2         mathematical modeller / simulation analyst / o...\n",
            "100002    a successful and high achieving specialist sch...\n",
            "200002    web designer html , css , javascript , photosh...\n",
            "Name: FullDescription, dtype: object\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "239768it [00:24, 9738.79it/s]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
        "# see task above\n",
        "def normalize(text):\n",
        "    text = str(text).lower()\n",
        "    return ' '.join(tokenizer.tokenize(text))\n",
        "    \n",
        "data[text_columns] = data[text_columns].applymap(normalize)\n",
        "\n",
        "print(\"Tokenized:\")\n",
        "print(data[\"FullDescription\"][2::100000])\n",
        "assert data[\"FullDescription\"][2][:50] == 'mathematical modeller / simulation analyst / opera'\n",
        "assert data[\"Title\"][54321] == 'international digital account manager ( german )'\n",
        "\n",
        "# Count how many times does each token occur in both \"Title\" and \"FullDescription\" in total\n",
        "# build a dictionary { token -> it's count }\n",
        "from collections import Counter\n",
        "from tqdm import tqdm as tqdm\n",
        "\n",
        "token_counts = Counter()# <YOUR CODE HERE>\n",
        "for _, row in tqdm(data[text_columns].iterrows()):\n",
        "    for string in row:\n",
        "        token_counts.update(string.split())\n",
        "\n",
        "# hint: you may or may not want to use collections.Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02LhQfWy1nKX",
        "outputId": "6573177c-0619-4100-9516-493621aa3c70"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2598827"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "token_counts.most_common(1)[0][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiOWbc15ycOb",
        "outputId": "a5cbdb0b-0db7-40dd-a256-6eff06ac5f9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique tokens : 201127\n",
            "('and', 2598827)\n",
            "('.', 2471477)\n",
            "(',', 2266256)\n",
            "('the', 2036428)\n",
            "('to', 1977039)\n",
            "...\n",
            "('dbms_stats', 1)\n",
            "('dbms_output', 1)\n",
            "('dbms_job', 1)\n",
            "Correct!\n",
            "Vocabulary size: 33795\n",
            "Correct!\n",
            "Correct!\n"
          ]
        }
      ],
      "source": [
        "print(\"Total unique tokens :\", len(token_counts))\n",
        "print('\\n'.join(map(str, token_counts.most_common(n=5))))\n",
        "print('...')\n",
        "print('\\n'.join(map(str, token_counts.most_common()[-3:])))\n",
        "\n",
        "assert token_counts.most_common(1)[0][1] in  range(2500000, 2700000)\n",
        "assert len(token_counts) in range(200000, 210000)\n",
        "print('Correct!')\n",
        "\n",
        "min_count = 10\n",
        "\n",
        "# tokens from token_counts keys that had at least min_count occurrences throughout the dataset\n",
        "tokens = [token for token, count in token_counts.items() if count >= min_count]# <YOUR CODE HERE>\n",
        "# Add a special tokens for unknown and empty words\n",
        "UNK, PAD = \"UNK\", \"PAD\"\n",
        "tokens = [UNK, PAD] + sorted(tokens)\n",
        "print(\"Vocabulary size:\", len(tokens))\n",
        "\n",
        "assert type(tokens) == list\n",
        "assert len(tokens) in range(32000, 35000)\n",
        "assert 'me' in tokens\n",
        "assert UNK in tokens\n",
        "print(\"Correct!\")\n",
        "\n",
        "token_to_id = {token: idx for idx, token in enumerate(tokens)}\n",
        "assert isinstance(token_to_id, dict)\n",
        "assert len(token_to_id) == len(tokens)\n",
        "for tok in tokens:\n",
        "    assert tokens[token_to_id[tok]] == tok\n",
        "\n",
        "print(\"Correct!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "JEsLeBjVycOw"
      },
      "outputs": [],
      "source": [
        "UNK_IX, PAD_IX = map(token_to_id.get, [UNK, PAD])\n",
        "\n",
        "def as_matrix(sequences, max_len=None):\n",
        "    \"\"\" Convert a list of tokens into a matrix with padding \"\"\"\n",
        "    if isinstance(sequences[0], str):\n",
        "        sequences = list(map(str.split, sequences))\n",
        "        \n",
        "    max_len = min(max(map(len, sequences)), max_len or float('inf'))\n",
        "    \n",
        "    matrix = np.full((len(sequences), max_len), np.int32(PAD_IX))\n",
        "    for i,seq in enumerate(sequences):\n",
        "        row_ix = [token_to_id.get(word, UNK_IX) for word in seq[:max_len]]\n",
        "        matrix[i, :len(row_ix)] = row_ix\n",
        "    \n",
        "    return matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiBlPkdKycOy",
        "outputId": "39febce5-4fd3-4e46-d0cd-77983d97f491"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lines:\n",
            "engineering systems analyst\n",
            "hr assistant\n",
            "senior ec & i engineer\n",
            "\n",
            "Matrix:\n",
            "[[10705 29830  2143     1     1]\n",
            " [14875  2817     1     1     1]\n",
            " [27345 10107    15 15069 10702]]\n"
          ]
        }
      ],
      "source": [
        "print(\"Lines:\")\n",
        "print('\\n'.join(data[\"Title\"][::100000].values), end='\\n\\n')\n",
        "print(\"Matrix:\")\n",
        "print(as_matrix(data[\"Title\"][::100000]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "DpOlBp7ZycO6",
        "outputId": "352d2994-574f-4b18-e881-928e90131427"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DictVectorizer(dtype=<class 'numpy.float32'>, sparse=False)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DictVectorizer(dtype=&lt;class &#x27;numpy.float32&#x27;&gt;, sparse=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DictVectorizer</label><div class=\"sk-toggleable__content\"><pre>DictVectorizer(dtype=&lt;class &#x27;numpy.float32&#x27;&gt;, sparse=False)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "from sklearn.feature_extraction import DictVectorizer\n",
        "\n",
        "# we only consider top-1k most frequent companies to minimize memory usage\n",
        "top_companies, top_counts = zip(*Counter(data['Company']).most_common(1000))\n",
        "recognized_companies = set(top_companies)\n",
        "data[\"Company\"] = data[\"Company\"].apply(lambda comp: comp if comp in recognized_companies else \"Other\")\n",
        "\n",
        "categorical_vectorizer = DictVectorizer(dtype=np.float32, sparse=False)\n",
        "categorical_vectorizer.fit(data[categorical_columns].apply(dict, axis=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk4jmtAYycO8"
      },
      "source": [
        "### The deep learning part\n",
        "\n",
        "Once we've learned to tokenize the data, let's design a machine learning experiment.\n",
        "\n",
        "As before, we won't focus too much on validation, opting for a simple train-test split.\n",
        "\n",
        "__To be completely rigorous,__ we've comitted a small crime here: we used the whole data for tokenization and vocabulary building. A more strict way would be to do that part on training set only. You may want to do that and measure the magnitude of changes.\n",
        "\n",
        "\n",
        "#### Here comes the simple one-headed network from the seminar. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TngLcWA0ycO_",
        "outputId": "0df6bdbb-9c41-42be-a919-e9f81b1af8a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size =  191814\n",
            "Validation size =  47954\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data_train, data_val = train_test_split(data, test_size=0.2, random_state=42)\n",
        "data_train.index = range(len(data_train))\n",
        "data_val.index = range(len(data_val))\n",
        "\n",
        "print(\"Train size = \", len(data_train))\n",
        "print(\"Validation size = \", len(data_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "2PXuKgOSycPB"
      },
      "outputs": [],
      "source": [
        "def make_batch(data, max_len=None, word_dropout=0):\n",
        "    \"\"\"\n",
        "    Creates a keras-friendly dict from the batch data.\n",
        "    :param word_dropout: replaces token index with UNK_IX with this probability\n",
        "    :returns: a dict with {'title' : int64[batch, title_max_len]\n",
        "    \"\"\"\n",
        "    batch = {}\n",
        "    batch[\"Title\"] = as_matrix(data[\"Title\"].values, max_len)\n",
        "    batch[\"FullDescription\"] = as_matrix(data[\"FullDescription\"].values, max_len)\n",
        "    batch['Categorical'] = categorical_vectorizer.transform(data[categorical_columns].apply(dict, axis=1))\n",
        "    \n",
        "    if word_dropout != 0:\n",
        "        batch[\"FullDescription\"] = apply_word_dropout(batch[\"FullDescription\"], 1. - word_dropout)\n",
        "    \n",
        "    if target_column in data.columns:\n",
        "        batch[target_column] = data[target_column].values\n",
        "    \n",
        "    return batch\n",
        "\n",
        "def apply_word_dropout(matrix, keep_prop, replace_with=UNK_IX, pad_ix=PAD_IX,):\n",
        "    dropout_mask = np.random.choice(2, np.shape(matrix), p=[keep_prop, 1 - keep_prop])\n",
        "    dropout_mask &= matrix != pad_ix\n",
        "    return np.choose(dropout_mask, [matrix, np.full_like(matrix, replace_with)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "I6LpEQf0ycPD"
      },
      "outputs": [],
      "source": [
        "a = make_batch(data_train[:3], max_len=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzWbYAYH1nKc"
      },
      "source": [
        "But to start with let's build the simple model using only the part of the data. Let's create the baseline solution using only the description part (so it should definetely fit into the Sequential model)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "C1Ga5CBs1nKd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "3gWtb5cx1nKd"
      },
      "outputs": [],
      "source": [
        "# You will need these to make it simple\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1)\n",
        "\n",
        "class Reorder(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return input.permute((0, 2, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnXXeTNd1nKd"
      },
      "source": [
        "To generate minibatches we will use simple pyton generator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "FQSAyrPt1nKe"
      },
      "outputs": [],
      "source": [
        "def iterate_minibatches(data, batch_size=256, shuffle=True, cycle=False, **kwargs):\n",
        "    \"\"\" iterates minibatches of data in random order \"\"\"\n",
        "    while True:\n",
        "        indices = np.arange(len(data))\n",
        "        if shuffle:\n",
        "            indices = np.random.permutation(indices)\n",
        "\n",
        "        for start in range(0, len(indices), batch_size):\n",
        "            batch = make_batch(data.iloc[indices[start : start + batch_size]], **kwargs)\n",
        "            target = batch.pop(target_column)\n",
        "            yield batch, target\n",
        "        \n",
        "        if not cycle: break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "8CVJIVzA1nKe"
      },
      "outputs": [],
      "source": [
        "iterator = iterate_minibatches(data_train, 3)\n",
        "batch, target = next(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "yuBQLIWG1nKe"
      },
      "outputs": [],
      "source": [
        "# Here is some startup code:\n",
        "n_tokens=len(tokens)\n",
        "n_cat_features=len(categorical_vectorizer.vocabulary_)\n",
        "hid_size=64\n",
        "simple_model = nn.Sequential()\n",
        "\n",
        "simple_model.add_module('emb', nn.Embedding(num_embeddings=n_tokens, embedding_dim=hid_size))\n",
        "simple_model.add_module('reorder', Reorder())\n",
        "simple_model.add_module('conv1', nn.Conv1d(\n",
        "    in_channels=hid_size,\n",
        "    out_channels=hid_size,\n",
        "    kernel_size=2)\n",
        "                       )\n",
        "simple_model.add_module('relu1', nn.ReLU())\n",
        "simple_model.add_module('conv2', nn.Conv1d(in_channels=hid_size, out_channels=hid_size, kernel_size=3))\n",
        "simple_model.add_module('relu2', nn.ReLU())\n",
        "simple_model.add_module('bnorm', nn.BatchNorm1d(hid_size))\n",
        "simple_model.add_module('adapt_avg_pool', nn.AdaptiveAvgPool1d(output_size=1))\n",
        "simple_model.add_module('flatten1', Flatten())\n",
        "simple_model.add_module('linear1', nn.Linear(in_features=hid_size, out_features=1))\n",
        "# <YOUR CODE HERE>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rraeuX-H1nKf",
        "outputId": "7c4c2f81-e25b-4662-adca-6e665e6e84d6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Title': array([[ 6480, 27483, 31383],\n",
              "        [19197, 22181, 10702],\n",
              "        [ 9438,  1175, 18670]], dtype=int32),\n",
              " 'FullDescription': array([[32718,  2545, 18235, 12466,   965, 30654, 27437,  6480, 27483,\n",
              "         31383, 33331, 21556,  3702, 21405,   965,  6835, 30407, 18771,\n",
              "          1597,  6948,   167, 15402,   965, 25451,  6291, 10866, 30411,\n",
              "         26324, 15446, 30411, 26563,   156, 27485,  2166, 21685,    80,\n",
              "            80,  2166,    80,  3213,  6480, 19627, 18488,    32, 32199,\n",
              "         21784, 14753,    71, 30828, 27485, 15447, 27485, 30828, 13214,\n",
              "           167, 27485,  2166,  5855, 21405, 12236, 31946, 22067,  7763,\n",
              "           156, 28100,   156,  5810, 11116,   167,  3523, 24071, 24912,\n",
              "          2166, 33635, 20357,  3607,  5263, 21405, 24000, 12177, 21444,\n",
              "         30762,  9762,  2166, 24569, 28555,   167, 33635, 33079,  1973,\n",
              "          3607, 25867, 12466, 18969,  7366,   156, 18590, 14839, 28850,\n",
              "          2166,  2832,  2166,  8998, 13412,  2166, 21293,   167, 14822,\n",
              "         21405, 33306,  2545,   125, 30762,   125,  2892,   965, 22495,\n",
              "         24818, 21405,    80, 22697, 14818,   167,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1],\n",
              "        [19197, 22181, 10702,   891, 33374,  6261,  8632, 21831,  3512,\n",
              "         15402, 29002,   156, 14386, 16289,  8091, 18235, 30762, 25104,\n",
              "           965, 19197, 22181, 10702,   167, 30411,  7338, 16289, 15707,\n",
              "         12466,   736, 19947,    32, 21590,  1650,    71, 30411, 14821,\n",
              "         24818, 16289,    80, 22697, 14818,   167, 33635,  9526, 21084,\n",
              "         20573, 30762, 14632,  8090, 26896, 27183,  6307,   912, 14860,\n",
              "         33635, 20357,  3607, 10423, 30762,  3607,  6310, 30762, 30512,\n",
              "         17758,   167, 19197, 22181, 10702, 16658,  8879,   891, 16378,\n",
              "         33403,  3607, 23686, 15187,  5223, 14109, 24021, 11453, 14860,\n",
              "         30487, 19037,  7191,    18, 11302,    18,  5223, 33198, 21950,\n",
              "            80,  8252, 22177,   167,  5223, 20573, 30762,  3607,    18,\n",
              "          8903,    18, 24822, 30401,    18,     0,    26, 25861,   891,\n",
              "          8652, 21405,  3836, 19197, 22181,  8909, 14845,     0,   167,\n",
              "         16027, 21405, 29382, 28555,  2166, 13169, 21405,    80,  8252,\n",
              "         19838,   195,  8909, 21556,  5055,    32, 24021,    71, 13169,\n",
              "         21405,  5595, 15976,  8653, 15447, 10387,  8341, 11308, 30762,\n",
              "         10111, 30842,   167,  5425, 21977, 16149, 16079,   965, 12850,\n",
              "         24786, 21405, 19197, 22181, 23956,   156, 16366, 21784,  9006,\n",
              "          2166,  8988,  2166, 23721, 28350,   156, 15543, 21784,  2662,\n",
              "           965, 19292, 21405,   965, 24093, 30080,   167, 29566,   195,\n",
              "         17816, 33198, 18776,   156, 25815, 18776, 23956,    15,  8631,\n",
              "           167, 29566, 30762, 12875,    15, 10868, 31212,   167, 17816,\n",
              "         33198, 29559,   195, 24409, 30762, 25811,  2311, 30131, 24624,\n",
              "         30407, 19037,  2568,   167, 23148, 22123,  8435,  1328,   167,\n",
              "         28011,   195, 11453,   891, 11453, 15402, 28553,   156,  8894,\n",
              "            15, 18771, 21405, 10387, 29383,   167, 17203, 21405, 30411,\n",
              "         12426,   891, 18776, 23975,   156, 10509,   156, 13053,   156,\n",
              "          7151, 30762,  4755,    15, 19615, 28555,   156, 30795, 29264,\n",
              "           156, 10868, 30343,   156, 30465, 18664,   156, 18971,   156,\n",
              "          5595,   195, 12288, 18771,   156, 10387,  6948,   167,  2120,\n",
              "          2433, 21405, 21950, 10705,  2548,   195,  9578,   167, 19197,\n",
              "         22181, 10702, 24021, 23935, 10004, 29002, 28444, 10065, 14386,\n",
              "         18192,  5160,  8632,  8638, 10705,  1597, 26896,  9957, 27183,\n",
              "          6307, 19728, 19729, 32727,  7338],\n",
              "        [16658, 30725,   891,  9438,  1175, 18670, 18132,   891, 14336,\n",
              "             0,   891,    80,    80,  6765,  5303,     0, 25861,   891,\n",
              "         16658, 24424,   891, 31523,  9438,  1175, 18670, 12466,  8922,\n",
              "           156, 33229,  2166, 17539, 24001, 17943, 27306, 21977, 21405,\n",
              "          9438,   167, 16130,  7366, 12319, 32545, 23850, 29630,   167,\n",
              "         13717, 21405,  9438, 19946, 23850, 18003,  9438, 12332, 33306,\n",
              "         19946, 23850, 29629,  9438, 16130,     0, 17203, 25722,   891,\n",
              "         15143, 27312, 16079,  9438, 18842, 13446, 17203, 21405, 16378,\n",
              "           195, 30131,     0, 28011,   891, 28011,   195, 24556,   891,\n",
              "         11295, 16020, 28011, 12850, 31523,  9813, 17845, 15994, 30762,\n",
              "         11295, 20145, 11288,     0,  6877,   891, 30411, 29406, 15538,\n",
              "         27736,  3607,  7089,  2166,  1048, 30762, 10689,  1297,  1894,\n",
              "         17768,  2166,  1894, 12880,   167, 13446, 32158,  2166, 33438,\n",
              "         28011, 25722,   167,   965, 29227,  5870,  1048, 30762, 20596,\n",
              "         10261,  1048, 30762, 22719, 10261, 32974, 16814,   965, 21196,\n",
              "         21405, 24114, 20357,  3607, 21852, 29894, 22129, 30080, 23189,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1]], dtype=int32),\n",
              " 'Categorical': array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.]], dtype=float32)}"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1vvVcPS1nKf"
      },
      "source": [
        "__Remember!__ We are working with regression problem and predicting only one number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DWFj48j1nKf",
        "outputId": "5bf5da3d-7631-4544-d68b-e808651101b0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0129],\n",
              "        [ 0.2865],\n",
              "        [-0.0061]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "source": [
        "# Try this to check your model. `torch.long` tensors are required for nn.Embedding layers.\n",
        "simple_model(torch.tensor(batch['FullDescription'], dtype=torch.long))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZNQ9EWt1nKg",
        "outputId": "97eb953e-14db-49e4-9a03-58f25e09f917"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 302)"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "batch['FullDescription'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now simple training pipeline (it's commented because we've already done that in class. No need to do it again)."
      ],
      "metadata": {
        "id": "wi0wtl65ArVV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "1FF_ZxNz1nKg"
      },
      "outputs": [],
      "source": [
        "# from IPython.display import clear_output\n",
        "# from random import sample\n",
        "\n",
        "# epochs = 1\n",
        "\n",
        "# model = simple_model\n",
        "# opt = torch.optim.Adam(model.parameters())\n",
        "# loss_func = nn.MSELoss()\n",
        "\n",
        "# history = []\n",
        "# for epoch_num in range(epochs):\n",
        "#     for idx, (batch, target) in enumerate(iterate_minibatches(data_train)):\n",
        "#         # Preprocessing the batch data and target\n",
        "#         batch = torch.tensor(batch['FullDescription'], dtype=torch.long)\n",
        "\n",
        "#         target = torch.tensor(target)\n",
        "\n",
        "\n",
        "#         predictions = model(batch)\n",
        "#         predictions = predictions.view(predictions.size(0))\n",
        "\n",
        "#         loss = loss_func(predictions, target)# <YOUR CODE HERE>\n",
        "\n",
        "#         # train with backprop\n",
        "#         loss.backward()\n",
        "#         opt.step()\n",
        "#         opt.zero_grad()\n",
        "#         # <YOUR CODE HERE>\n",
        "\n",
        "#         history.append(loss.data.numpy())\n",
        "#         if (idx+1)%10==0:\n",
        "#             clear_output(True)\n",
        "#             plt.plot(history,label='loss')\n",
        "#             plt.legend()\n",
        "#             plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SizXbM551nKh"
      },
      "source": [
        "### Actual homework starts here\n",
        "__Your ultimate task is to code the three headed network described on the picture below.__ \n",
        "To make it closer to the real world, please store the network code in file `network.py` in this directory. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eI5h9UMycPF"
      },
      "source": [
        "#### Architecture\n",
        "\n",
        "Our main model consists of three branches:\n",
        "* Title encoder\n",
        "* Description encoder\n",
        "* Categorical features encoder\n",
        "\n",
        "We will then feed all 3 branches into one common network that predicts salary.\n",
        "\n",
        "<img src=\"https://github.com/yandexdataschool/nlp_course/raw/master/resources/w2_conv_arch.png\" width=600px>\n",
        "\n",
        "This clearly doesn't fit into PyTorch __Sequential__ interface. To build such a network, one will have to use [__PyTorch nn.Module API__](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "Gx2KoJiD1nKj"
      },
      "outputs": [],
      "source": [
        "import network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qInRNQjj1nKk",
        "outputId": "52b1a2ea-f038-40c5-8491-158062dd4144"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'network' from '/content/network.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ],
      "source": [
        "# Re-run this cell if you updated the file with network source code\n",
        "import imp\n",
        "imp.reload(network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "0zqAzria1nKl"
      },
      "outputs": [],
      "source": [
        "model = network.ThreeInputsNet(\n",
        "    n_tokens=len(tokens),\n",
        "    n_cat_features=len(categorical_vectorizer.vocabulary_),\n",
        "\n",
        "    # this parameter defines the number of the inputs in the layer,\n",
        "    # which stands after the concatenation. In should be found out by you.\n",
        "    concat_number_of_features=6*hid_size,\n",
        "    hid_size=hid_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "uIz_3bIG1nKm"
      },
      "outputs": [],
      "source": [
        "testing_batch, _ = next(iterate_minibatches(data_train, 3))\n",
        "testing_batch = [\n",
        "    torch.tensor(testing_batch['Title'], dtype=torch.long),\n",
        "    torch.tensor(testing_batch['FullDescription'], dtype=torch.long),\n",
        "    torch.tensor(testing_batch['Categorical'])\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqBGCvHN1nKm",
        "outputId": "e678fe73-7f35-4377-cee6-cbdf2b17f7fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seems fine!\n"
          ]
        }
      ],
      "source": [
        "assert model(testing_batch).shape == torch.Size([3, 1])\n",
        "assert model(testing_batch).dtype == torch.float32\n",
        "print('Seems fine!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgprGC761nKm"
      },
      "source": [
        "Now train the network for a while (100 batches would be fine)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6TPeXB11nKn"
      },
      "outputs": [],
      "source": [
        "# Training pipeline comes here (almost the same as for the simple_model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "from random import sample\n",
        "\n",
        "epochs = 1\n",
        "\n",
        "opt = torch.optim.Adam(model.parameters())\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "history = []\n",
        "for epoch_num in range(epochs):\n",
        "    for idx, (batch, target) in enumerate(iterate_minibatches(data_train)):\n",
        "        # Preprocessing the batch data and target\n",
        "        batch = [\n",
        "            torch.tensor(batch['Title'], dtype=torch.long),\n",
        "            torch.tensor(batch['FullDescription'], dtype=torch.long),\n",
        "            torch.tensor(batch['Categorical'])\n",
        "        ]\n",
        "\n",
        "        target = torch.tensor(target)\n",
        "        \n",
        "        predictions = model(batch)\n",
        "        predictions = predictions.view(predictions.size(0))\n",
        "\n",
        "        loss = loss_func(predictions, target)\n",
        "\n",
        "        # train with backprop\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "        history.append(loss.data.numpy())\n",
        "        if (idx+1)%10==0:\n",
        "            clear_output(True)\n",
        "            plt.plot(history,label='loss')\n",
        "            plt.legend()\n",
        "            plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "e8BZVrrKFgDS",
        "outputId": "4afbacb1-ba7f-499a-8add-0751b9fdc594"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqvyKqkR1nKo"
      },
      "source": [
        "Now, to evaluate the model it can be switched to `eval` state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hd1XxMkl1nKo",
        "outputId": "1745525f-26d7-4a12-dac8-40888d109528"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ThreeInputsNet(\n",
              "  (title_emb): Embedding(33795, 64)\n",
              "  (title_conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,))\n",
              "  (title_pool): AdaptiveMaxPool1d(output_size=1)\n",
              "  (full_emb): Embedding(33795, 64)\n",
              "  (full_conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,))\n",
              "  (full_conv2): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(1,))\n",
              "  (full_conv3): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(2,))\n",
              "  (full_pool): AdaptiveMaxPool1d(output_size=1)\n",
              "  (category_out): Linear(in_features=3746, out_features=128, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (inter_dense): Linear(in_features=384, out_features=128, bias=True)\n",
              "  (final_dense): Linear(in_features=128, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "-mIixVZx1nKp"
      },
      "outputs": [],
      "source": [
        "def generate_submission(model, data, batch_size=256, name=\"\", three_inputs_mode=True, **kw):\n",
        "    squared_error = abs_error = num_samples = 0.0\n",
        "    output_list = []\n",
        "    for batch_x, batch_y in tqdm(iterate_minibatches(data, batch_size=batch_size, shuffle=False, **kw)):\n",
        "        if three_inputs_mode:\n",
        "            batch = [\n",
        "                torch.tensor(batch_x['Title'], dtype=torch.long),\n",
        "                torch.tensor(batch_x['FullDescription'], dtype=torch.long),\n",
        "                torch.tensor(batch_x['Categorical'])\n",
        "            ]\n",
        "        else:\n",
        "            batch = torch.tensor(batch_x['FullDescription'], dtype=torch.long)\n",
        "\n",
        "        batch_pred = model(batch)[:, 0].detach().numpy()\n",
        "        \n",
        "        output_list.append((list(batch_pred), list(batch_y)))\n",
        "        \n",
        "        squared_error += np.sum(np.square(batch_pred - batch_y))\n",
        "        abs_error += np.sum(np.abs(batch_pred - batch_y))\n",
        "        num_samples += len(batch_y)\n",
        "    print(\"%s results:\" % (name or \"\"))\n",
        "    print(\"Mean square error: %.5f\" % (squared_error / num_samples))\n",
        "    print(\"Mean absolute error: %.5f\" % (abs_error / num_samples))\n",
        "    \n",
        "\n",
        "    batch_pred = [c for x in output_list for c in x[0]]\n",
        "    batch_y = [c for x in output_list for c in x[1]]\n",
        "    output_df = pd.DataFrame(list(zip(batch_pred, batch_y)), columns=['batch_pred', 'batch_y'])\n",
        "    output_df.to_csv('submission.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_p0pMzB1nKq",
        "outputId": "6ad56589-532e-4706-86f8-3d1fdbb5f8c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "20it [00:18,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission results:\n",
            "Mean square error: 0.14680\n",
            "Mean absolute error: 0.29882\n",
            "Submission file generated\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "generate_submission(model, data_for_autotest, name='Submission')\n",
        "print('Submission file generated')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUi83Z0W1nKq"
      },
      "source": [
        "__Both the notebook and the `.py` file are required to submit this homework.__"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}